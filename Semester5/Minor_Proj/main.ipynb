{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federated Learning On Distributed Private Health Data On Smartphones\n",
    "\n",
    "The **smartphones of the people probably carry the most valueable but also private data**. Since using data promisses to be one of the best ways to fight back against COVID-19, it is highly desirable to get access.\n",
    "\n",
    "\n",
    "By using a **[Federated Learning](https://federated.withgoogle.com/)** approach with PySyft it is possible to **learn from the private data right on the smartphone, with the data never leaving the device**.\n",
    "\n",
    "\n",
    "# Approach\n",
    "\n",
    "1. Since there is no private dataset with health data during a virus outbreak, a **simulated dataset has been used to show the prove of concept**.\n",
    "2. The **dataset contains the health status of each person** (e.g. temperature, movement, ... ) for several days during the virus outbreak.\n",
    "3. Using PySyft-Workers the **data for each single person is distributed to a worker** (virtual smartphone). Therefor each worker only knows its own health status.\n",
    "4. A **simple feedforward network is send to each worker during the training process**. The learning takes place directly on the virtual smartphone itself and an updated network is returned to the host. This way the data did never leave the smartphone and stays protected. \n",
    "5. The **target variable to predict is the total number of infected people** in this notebook.\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "It is **possible to make us of the private health data of the people without lowering the protection of the data**.  \n",
    "The notebook can be seen as prove of concept that learning on distributed individual health data can start a learning process in neural network. \n",
    "\n",
    "\n",
    "# Limitations (With Possible Solutions)\n",
    "\n",
    "**Limitation:**  \n",
    "Simulated data without a connection to the real world has been used.  \n",
    "**Solution:**  \n",
    "Exchanging the dataset and adjusting the code should be pretty easy.\n",
    "\n",
    "\n",
    "**Limitation:**  \n",
    "A trusted App with permission to store private health data on the device is needed on many smartphones.  \n",
    "**Solution:**  \n",
    "Probably another team created a similar App during the hackathon or there is an existing one already out there. Merging this approach with such an App is necessary. \n",
    "\n",
    "\n",
    "**Limitation:**  \n",
    "The hardware limitations have been very strict for this notebook.  \n",
    "**Solution:**  \n",
    "Running the simulation and training process on a much larger scale should indicate if the approach is promising.\n",
    "\n",
    "\n",
    "\n",
    "# 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T15:07:51.687203Z",
     "start_time": "2020-03-22T15:07:49.605589Z"
    }
   },
   "outputs": [],
   "source": [
    "# To comunicate with decentralised devices\n",
    "import syft as sy\n",
    "\n",
    "# To do linear algebra\n",
    "import numpy as np\n",
    "\n",
    "# To store data\n",
    "import pandas as pd\n",
    "\n",
    "# To get progression bars\n",
    "from tqdm import tqdm\n",
    "\n",
    "# To create neural networks\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# To create plots\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# To transform the input data\n",
    "from torchvision import transforms\n",
    "\n",
    "# To standardize the dataset\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Setup Variables, Functions And Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T15:07:51.708380Z",
     "start_time": "2020-03-22T15:07:51.689090Z"
    }
   },
   "outputs": [],
   "source": [
    "# Arguments for learning process\n",
    "class Arguments():\n",
    "    def __init__(self):\n",
    "        self.batch_size = 64\n",
    "        self.test_batch_size = 1000\n",
    "        self.epochs = 10\n",
    "        self.lr = 0.01\n",
    "        self.momentum = 0.5\n",
    "        self.no_cuda = False\n",
    "        self.seed = 1\n",
    "        self.log_interval = 1000\n",
    "        self.save_model = False\n",
    "\n",
    "args = Arguments()\n",
    "\n",
    "# Use CUDA\n",
    "use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "# Set seed for computation\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "# Create CUDA device\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "# Additional arguments for CUDA\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
    "\n",
    "\n",
    "# Define neural network\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(5, 20)\n",
    "        self.fc2 = nn.Linear(20, 20)\n",
    "        self.fc3 = nn.Linear(20, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Load And Prepare The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T15:08:09.881667Z",
     "start_time": "2020-03-22T15:07:51.710945Z"
    }
   },
   "outputs": [],
   "source": [
    "########## Create Training Dataset ##########\n",
    "# Load the training data\n",
    "df_train = pd.read_csv('data/Trainingset_Simulated_Virus_Outbreak.csv')\n",
    "\n",
    "# Get number of toal infected people per day as prediction label\n",
    "total_infected = df_train.groupby('Day')['Infected'].sum()\n",
    "\n",
    "# Add the target column\n",
    "df_train['Target'] = df_train['Day'].apply(lambda x: total_infected.loc[x])\n",
    "\n",
    "# Transform the regression target \n",
    "df_train['Target'] = df_train['Target'].apply(np.log10)\n",
    "\n",
    "# Transform the data with a standard scaler\n",
    "sc = StandardScaler()\n",
    "df_train[['Temperature', 'Distance']] = sc.fit_transform(df_train[['Temperature', 'Distance']])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########## Create Testing Dataset ##########\n",
    "# Load the testing data\n",
    "df_test = pd.read_csv('data/Testingset_Simulated_Virus_Outbreak.csv')\n",
    "\n",
    "# Get number of toal infected people per day as prediction label\n",
    "total_infected = df_test.groupby('Day')['Infected'].sum()\n",
    "\n",
    "# Add the target column\n",
    "df_test['Target'] = df_test['Day'].apply(lambda x: total_infected.loc[x])\n",
    "\n",
    "# Transform the regression target \n",
    "df_test['Target'] = df_test['Target'].apply(np.log10)\n",
    "\n",
    "# Transform the data with a standard scaler\n",
    "df_test[['Temperature', 'Distance']] = sc.transform(df_test[['Temperature', 'Distance']])\n",
    "\n",
    "df_test.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Start The Workers And Distribute The Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T15:08:17.058670Z",
     "start_time": "2020-03-22T15:08:09.883782Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a hook to devices\n",
    "hook = sy.TorchHook(torch)\n",
    "\n",
    "# Transforms for the data\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "########## Create Training Workers And Dataset ##########\n",
    "# Number of nodes (Fewer nodes are used since hardware restrictions; all nodes would be optimal)\n",
    "n = 500 # df['Node'].nunique()\n",
    "\n",
    "# Create a virtual training worker for each simulated device\n",
    "training_workers = [sy.VirtualWorker(hook, id=node) for node in range(n)]\n",
    "\n",
    "\n",
    "# Store all distributed datasets\n",
    "datasets_train = []\n",
    "datasets_test = []\n",
    "\n",
    "# Iterate over all training workers\n",
    "for i, worker in tqdm(enumerate(training_workers)):\n",
    "    \n",
    "    # Filter for the workers specific training dataset\n",
    "    y = df_train[df_train['Node']==i]['Target'].values\n",
    "    X = df_train[df_train['Node']==i].drop(['Day', 'Node', 'Target'], axis=1).values\n",
    "\n",
    "    # Transform data to tensors\n",
    "    y = torch.tensor(y)\n",
    "    X = torch.tensor([values.astype(float) for values in X])\n",
    "    \n",
    "    # Store the dataset\n",
    "    datasets_train.append(sy.BaseDataset(X, y, transform=transform).send(worker))\n",
    "    \n",
    "    \n",
    "    # Filter for the workers specific testing dataset\n",
    "    y = df_test[df_test['Node']==i]['Target'].values\n",
    "    X = df_test[df_test['Node']==i].drop(['Day', 'Node', 'Target'], axis=1).values\n",
    "\n",
    "    # Transform data to tensors\n",
    "    y = torch.tensor(y)\n",
    "    X = torch.tensor([values.astype(float) for values in X])\n",
    "    \n",
    "    # Store the dataset\n",
    "    datasets_test.append(sy.BaseDataset(X, y, transform=transform).send(worker))\n",
    "\n",
    "\n",
    "\n",
    "# Create a federated dataset\n",
    "dataset_train_federated = sy.FederatedDataset(datasets_train)\n",
    "\n",
    "# Create a federated loader\n",
    "dataloader_train_federated = sy.FederatedDataLoader(dataset_train_federated, shuffle=True, batch_size=10)\n",
    "\n",
    "\n",
    "# Create a federated dataset\n",
    "dataset_test_federated = sy.FederatedDataset(datasets_test)\n",
    "\n",
    "# Create a federated loader\n",
    "dataloader_test_federated = sy.FederatedDataLoader(dataset_test_federated, shuffle=True, batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T00:11:10.879017Z",
     "start_time": "2020-03-22T00:11:10.874797Z"
    }
   },
   "source": [
    "# 5. Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T15:08:17.071376Z",
     "start_time": "2020-03-22T15:08:17.059867Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define training process\n",
    "def train(args, model, device, federated_train_loader, optimizer, epoch):\n",
    "    \n",
    "    # Accumulate total loss\n",
    "    total_loss = []\n",
    "    \n",
    "    # \n",
    "    model.train()\n",
    "    \n",
    "    # Iterate over the federated training datasets\n",
    "    for batch_idx, (data, target) in enumerate(federated_train_loader):\n",
    "        \n",
    "        # Send the model to the worker of the training batch\n",
    "        model.send(data.location)\n",
    "        \n",
    "        # Move the training tensors to CUDA device\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        # Set the gradients to zero\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Pass the data through the model\n",
    "        output = model(data.float())\n",
    "        \n",
    "        # Compute the training loss\n",
    "        #loss = nn.MSELoss(output, target.long())\n",
    "        loss = nn.MSELoss()(output, target.float())\n",
    "        \n",
    "        # Backpropagate the loss through the network\n",
    "        loss.backward()\n",
    "        \n",
    "        # Perform an update of the network\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Return the model\n",
    "        model.get()\n",
    "        \n",
    "        # Return the loss\n",
    "        loss = loss.get()\n",
    "        total_loss.append(loss)\n",
    "        \n",
    "        # Check to create new log entry (Stopped printing for github)\n",
    "        if batch_idx % args.log_interval == 0 or False:\n",
    "            \n",
    "            # Update training process\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(epoch, \n",
    "                                                                           batch_idx * args.batch_size, \n",
    "                                                                           len(federated_train_loader) * args.batch_size,\n",
    "                                                                           100. * batch_idx / len(federated_train_loader), \n",
    "                                                                           loss.item()))\n",
    "        \n",
    "    # Retrieve data from tensors\n",
    "    total_loss = [tensor.cpu().data.numpy() for tensor in total_loss]\n",
    "    \n",
    "    # Return mean loss per batch\n",
    "    mean_loss = np.mean(total_loss) \n",
    "    \n",
    "    # Display mean loss per batch\n",
    "    print('Train Mean Batch Loss: {}'.format(mean_loss))\n",
    "    return mean_loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define testing process\n",
    "def test(args, model, device, federated_test_loader):\n",
    "    \n",
    "    # Accumulate total loss\n",
    "    total_loss = []\n",
    "    \n",
    "    # Iterate over the federated training datasets\n",
    "    for batch_idx, (data, target) in enumerate(federated_test_loader):\n",
    "        \n",
    "        # Send the model to the worker of the testing batch\n",
    "        model.send(data.location)\n",
    "        \n",
    "        # Move the testing tensors to CUDA device\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        # Pass the data through the model\n",
    "        output = model(data.float())\n",
    "        \n",
    "        # Compute and store the testing loss\n",
    "        loss = nn.MSELoss()(output, target.float())\n",
    "        total_loss.append(loss.get())\n",
    "        \n",
    "        # Return the model\n",
    "        model.get()\n",
    "    \n",
    "    \n",
    "    # Retrieve data from tensors\n",
    "    total_loss = [tensor.cpu().data.numpy() for tensor in total_loss]\n",
    "    \n",
    "    # Return mean loss per batch\n",
    "    mean_loss = np.mean(total_loss) \n",
    "    \n",
    "    # Display mean loss per batch\n",
    "    print('Test Mean Batch Loss: {}\\n'.format(mean_loss))\n",
    "    return mean_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T15:43:40.096285Z",
     "start_time": "2020-03-22T15:08:17.072892Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Create the neural network\n",
    "model = Net().to(device)\n",
    "\n",
    "# Create an optimizer for the network\n",
    "optimizer = optim.SGD(model.parameters(), lr=args.lr)\n",
    "\n",
    "\n",
    "# Store losses for training inspection\n",
    "training_losses = []\n",
    "testing_losses = []\n",
    "\n",
    "# Iterate over all epochs\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    \n",
    "    # Perform the training\n",
    "    mean_training_loss = train(args, model, device, dataloader_train_federated, optimizer, epoch)\n",
    "    \n",
    "    # Perform the testing\n",
    "    mean_testing_loss = test(args, model, device, dataloader_test_federated)\n",
    "    \n",
    "    # Store losses\n",
    "    training_losses.append(mean_training_loss)\n",
    "    testing_losses.append(mean_testing_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Inspect Training Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T15:43:40.492536Z",
     "start_time": "2020-03-22T15:43:40.098004Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a plot for training inspection\n",
    "fig, axs = plt.subplots(2, figsize=(15, 6))\n",
    "\n",
    "# Training losses\n",
    "axs[0].plot(range(len(training_losses)), training_losses)\n",
    "axs[0].set_title('Training Losses')\n",
    "axs[0].set_xlabel('Epochs')\n",
    "axs[0].set_ylabel('MSE')\n",
    "\n",
    "# Testing losses\n",
    "axs[1].plot(range(len(testing_losses)), testing_losses)\n",
    "axs[1].set_title('Testing Losses')\n",
    "axs[1].set_xlabel('Epochs')\n",
    "axs[1].set_ylabel('MSE')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('build/Inspecting_Training_Process.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
